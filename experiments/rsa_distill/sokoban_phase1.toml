# RSA-Distill Phase 1: Sokoban GRPO (multinode)
#
# Self-aggregation env: K student + K teacher responses per problem (2K total).
# Each teacher j generates from leave-one-out aggregation of K-1 student responses (excl. j).
# Per-step GRPO: student and teacher groups each have their own baseline.
# No teacher model, no KL distillation.
#
# Run on 2 nodes:
#   sbatch -A m4881 -C "gpu&hbm80g" --qos=premium --time 24:00:00 --gpus-per-node 4 --nodes=2 \
#     ~/prime_rl_rsa_distill_multinode.sh phase1
#
# __INFERENCE_NODE__ placeholder replaced by launch script.

# ============================================================================
# Training
# ============================================================================
max_steps = 100
seq_len = 32768             # Aggregation prompts can be long (K=4 * max_tokens=4096 responses)
output_dir = "outputs/rsa_distill_sokoban_phase1"

# ============================================================================
# Model
# ============================================================================
[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

# ============================================================================
# Checkpointing
# ============================================================================
[ckpt]
interval = 50

# ============================================================================
# W&B Logging
# ============================================================================
[wandb]
project = "rsa-distill"
name = "sokoban-phase1-grpo-k4"

# ============================================================================
# Trainer Config
# ============================================================================
[trainer.model]
impl = "hf"                    # liger_kernel disables fused_lm_head chunking → OOM at 32K seq_len

[trainer.model.ac]
freq = 1

[trainer.model.lora]
rank = 32
alpha = 64
train_lm_head = true

[trainer.ckpt.weights]
save_adapter_separately = true

[trainer.optim]
lr = 1e-5
weight_decay = 0.0
betas1 = 0.9
betas2 = 0.9
max_norm = 1.0

# ============================================================================
# Orchestrator Config
# ============================================================================
[orchestrator]
batch_size = 32             # 8 problems x 4 rollouts (smaller for smoke test)
rollouts_per_example = 4    # >= 2 for GRPO signal
trajectory_strategy = "branching"  # Each step = independent training sample

# Point orchestrator to inference server on Node 1
# __INFERENCE_NODE__ is replaced at launch time by the launch script
[orchestrator.client]
base_url = ["http://__INFERENCE_NODE__:8000/v1"]

[orchestrator.sampling]
max_tokens = 4096
temperature = 1.0

# RSA-Distill env: K student + K teacher responses (2K total, leave-one-out aggregation)
[[orchestrator.env]]
id = "rsa_distill"
args = { inner_env_id = "sokoban-env", inner_env_args = { num_train_examples = 15000, num_eval_examples = 1000, seed = 40, min_w = 4, max_w = 9, min_h = 4, max_h = 9, min_boxes = 1, max_boxes = 7, max_depth = 80 }, K = 4, task = "rg", per_step_grpo = true, enable_distill = false }

# ============================================================================
# No [inference] section — inference runs on Node 1.
# ============================================================================

# ============================================================================
# Eval Config (disabled for smoke test)
# ============================================================================
[orchestrator.eval]
num_examples = 32
rollouts_per_example = 1
eval_base_model = false
interval = 10000            # Never triggers during smoke test
