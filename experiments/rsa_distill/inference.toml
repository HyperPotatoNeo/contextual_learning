# Inference config for Node 1 (multi-node setup)
# Run with: CUDA_VISIBLE_DEVICES=0,1,2,3 uv run inference @ inference.toml
#
# 4 GPUs with DP=4 (4 independent vLLM workers behind single API on port 8000)

seed = 0

[server]
host = "0.0.0.0"   # Bind to all interfaces so Node 2 can reach us
port = 8000

[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

[parallel]
dp = 4
tp = 1
