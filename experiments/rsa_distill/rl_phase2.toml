# RSA-Distill Phase 2: GRPO + Bidirectional KL Distillation
#
# Self-aggregation env with context distillation:
# - Student completions: KL toward aggregation distribution (teacher logprobs)
# - Teacher completion: KL toward raw question distribution (teacher logprobs)
# - Bidirectional via prompt_text override (enable_distill=true)
#
# Teacher model = same as inference model (share_teacher_weights=true).
# KL applied in loss (use_full_reward_baseline=false) with weak tau.
# kl_only_incorrect gates KL on problems where students fail.
#
# Run with:
#   export PYTHONPATH=/pscratch/sd/s/siddart2/rsa_distill:/pscratch/sd/s/siddart2/rsa:$PYTHONPATH
#   uv run rl @ experiments/rl_phase2.toml \
#     --inference_gpu_ids 0,1 --trainer_gpu_ids 2,3 \
#     --inference.parallel.dp 2

# ============================================================================
# Training
# ============================================================================
max_steps = 500
seq_len = 16384
output_dir = "outputs/rsa_distill_phase2"

# ============================================================================
# Model
# ============================================================================
[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

# ============================================================================
# Checkpointing
# ============================================================================
[ckpt]
interval = 50

# ============================================================================
# W&B Logging
# ============================================================================
[wandb]
project = "rsa-distill"
name = "phase2-kl-k4-countdown"

# ============================================================================
# Trainer Config
# ============================================================================
[trainer.model]
impl = "liger_kernel"

[trainer.model.ac]
freq = 1

[trainer.model.lora]
rank = 32
alpha = 64
train_lm_head = true

[trainer.ckpt.weights]
save_adapter_separately = true

[trainer.optim]
lr = 1e-5
weight_decay = 0.0
betas1 = 0.9
betas2 = 0.9
max_norm = 1.0

# KL loss config: weak tau + gating on incorrect
[trainer.loss]
adv_tau = 1.0
teacher_tau = 0.001           # Weak KL signal (strong KL destroys perf)
student_tau = 0.001
kl_tau = 0.0                  # No extra KL on importance ratio

# ============================================================================
# Orchestrator Config
# ============================================================================
[orchestrator]
batch_size = 64
rollouts_per_example = 4
trajectory_strategy = "branching"

[orchestrator.sampling]
max_tokens = 4096
temperature = 1.0

# Advantage: no full_reward_baseline (KL applied in loss instead)
[orchestrator.advantage]
use_full_reward_baseline = false
kl_only_incorrect = true       # Gate KL on student accuracy

# Teacher model: same model, shares weights, neutral context
[orchestrator.teacher_model]
context = "You are a helpful assistant."
share_teacher_weights = true

# RSA-Distill env: enable_distill=true for prompt_text overrides
[[orchestrator.env]]
id = "rsa_distill"
args = { inner_env_id = "countdown", inner_env_args = { num_train_examples = 5000, num_eval_examples = 500, seed = 42 }, K = 4, task = "rg", per_step_grpo = true, enable_distill = true }

# ============================================================================
# Inference Config
# ============================================================================
[inference]
# gpu_memory_utilization = 0.9

# ============================================================================
# Eval Config
# ============================================================================
[orchestrator.eval]
num_examples = 64
rollouts_per_example = 1
eval_base_model = false
interval = 50

[orchestrator.eval.sampling]
max_tokens = 4096
temperature = 1.0

[[orchestrator.eval.env]]
id = "rsa_distill"
args = { inner_env_id = "countdown", inner_env_args = { num_train_examples = 5000, num_eval_examples = 500, seed = 43 }, K = 4, task = "rg", per_step_grpo = true, enable_distill = false }
