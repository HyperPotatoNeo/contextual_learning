# Sokoban RL Training Config
# Run with: uv run rl @ experiments/sokoban/rl.toml

# ============================================================================
# Training
# ============================================================================
max_steps = 500
seq_len = 4096              # Max sequence length (also micro batch token capacity)
output_dir = "outputs/sokoban"

# ============================================================================
# Model
# ============================================================================
[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

# ============================================================================
# Checkpointing
# ============================================================================
[ckpt]
interval = 50              # Save every N steps
# keep_last = 3              # Keep last N checkpoints

# ============================================================================
# W&B Logging
# ============================================================================
[wandb]
project = "sokoban-rl"
name = "sokoban-4b-lora"
# offline = true           # Set to true to disable W&B

# ============================================================================
# Trainer Config
# ============================================================================
[trainer.model]
impl = "liger_kernel"       # Optimized kernels

[trainer.model.ac]          # Activation checkpointing
freq = 1

[trainer.model.lora]        # LoRA settings
rank = 16
alpha = 64

[trainer.ckpt.weights]
save_adapter_separately = true

# dropout = 0.0
# target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

[trainer.optim]
lr = 1e-5
weight_decay = 0.0
max_norm = 1.0              # Gradient clipping

# [trainer.scheduler]
# type = "cosine"
# warmup_steps = 10

# ============================================================================
# Orchestrator Config
# ============================================================================
# RL Batching: Samples are sequence-packed into micro batches of seq_len tokens.
# Each micro batch has shape [1, seq_len]. Number of micro batches per GPU is
# approximately batch_size / dp_world_size (where dp_world_size = num trainer GPUs).
# There is no explicit micro_batch_size - it's computed dynamically by the packer.
[orchestrator]
batch_size = 256            # Global batch size (total samples per step)
rollouts_per_example = 8    # Rollouts per prompt

[orchestrator.sampling]
max_tokens = 4096           # Max generation tokens
temperature = 1.0           # Sampling temperature
# repetition_penalty = 1.0
# seed = 42                 # For reproducibility

[[orchestrator.env]]
id = "sokoban-env"
args = { num_train_examples = 5000, num_eval_examples = 200, seed = 42, min_w = 4, max_w = 8, min_h = 4, max_h = 8, min_boxes = 3, max_boxes = 8, max_depth = 80 }

# ============================================================================
# Inference Config
# ============================================================================
[inference]
# gpu_memory_utilization = 0.9

# [inference.parallel]
# tp = 1                    # Tensor parallelism for large models
