# Context Distillation + Task Reward (Hybrid)
# Run with: uv run rl @ experiments/context_distill/rl.toml
#
# This config demonstrates reverse-KL on-policy context distillation:
# - Teacher model sees enhanced prompt (with additional context/instructions)
# - Student model sees base prompt
# - Reward = task_reward + teacher_kl (hybrid mode)
#
# GPU allocation (4 GPUs):
#   GPU 0: Student inference (vLLM) - generates rollouts
#   GPU 1: Teacher inference (vLLM) - computes teacher logprobs with context
#   GPU 2-3: Trainer (FSDP2) - policy optimization
#
# Run with:
#   uv run rl @ experiments/context_distill/rl.toml \
#     --inference_gpu_ids 0 \
#     --teacher_gpu_ids 1 \
#     --trainer_gpu_ids 2 3

# ============================================================================
# Training
# ============================================================================
max_steps = 500
seq_len = 4096              # Max sequence length (also micro batch token capacity)
output_dir = "outputs/context_distill"

# ============================================================================
# Model
# ============================================================================
[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

# ============================================================================
# Checkpointing
# ============================================================================
[ckpt]
interval = 50              # Save every N steps

# ============================================================================
# W&B Logging
# ============================================================================
[wandb]
project = "sokoban-rl"
name = "sokoban-context-distill"

# ============================================================================
# Trainer Config
# ============================================================================
[trainer.model]
impl = "liger_kernel"       # Optimized kernels

[trainer.model.ac]          # Activation checkpointing
freq = 1

[trainer.model.lora]        # LoRA settings
rank = 16
alpha = 64

[trainer.ckpt.weights]
save_adapter_separately = true

[trainer.optim]
lr = 1e-5
weight_decay = 0.0
max_norm = 1.0              # Gradient clipping

# Hybrid loss: combine task reward + teacher/student log prob terms
# advantage = adv_tau * task_reward + teacher_tau * log(p_teacher) - student_tau * log(p_student)
[trainer.loss]
adv_tau = 0.5               # Weight for task reward
teacher_tau = 0.5           # Weight for teacher log prob term (distillation signal)
student_tau = 0.5           # Weight for student log prob term (entropy bonus)

# ============================================================================
# Orchestrator Config
# ============================================================================
[orchestrator]
batch_size = 256            # Global batch size (total samples per step)
rollouts_per_example = 8    # Rollouts per prompt

[orchestrator.advantage]
use_full_reward_baseline = true 

[orchestrator.sampling]
max_tokens = 4096           # Max generation tokens
temperature = 1.0           # Sampling temperature

# Teacher model config (same model, different prompt via context distillation)
# The teacher sees the student prompt with additional context prepended.
# This enables the student to learn to infer the behavior implied by the context.
[orchestrator.teacher_model]
context = """Given the fields `question`, produce the fields `reasoning`, `answer`.

You are solving Sokoban puzzles. Your goal is to find a sequence of moves (L=Left, R=Right, U=Up, D=Down) that places ALL boxes onto ALL goals.

**Symbol Key:**
- `*` - The player
- `%` - The player standing on a goal
- `@` - A box (not on a goal)
- `X` - A goal (empty, no box on it)
- `$` - A box on a goal
- `+` - A wall
- `-` - An empty position

**Critical Rules:**
1. The puzzle is solved when EVERY goal has a box on it. Count all goals (X, $, %) and all boxes (@, $) - they must match and all boxes must be on goals.
2. A `$` means a box is already on a goal at that position - this is good, but there may be OTHER goals (X) that still need boxes.
3. A `%` means the player is standing on a goal - that goal still needs a box pushed onto it.
4. To push a box, the player must be adjacent to it and move INTO the box's cell. The box moves one cell in the same direction. The cell behind the box (in the push direction) must be empty (-) or a goal (X).
5. Boxes cannot be pushed through walls or other boxes.
6. The player cannot walk through walls or boxes.

**Grid Parsing - CRITICAL:**
- Parse the grid carefully, accounting for spacing. Each cell is typically separated by spaces.
- Use 0-indexed coordinates: (row, column) where row 0 is the top row.
- When the grid has spaces between characters, each character position corresponds to even indices (0, 2, 4...) when reading the raw line, OR simply parse by splitting on whitespace.

**Solution Strategy:**
1. First, carefully parse the grid and identify ALL positions of:
   - Goals: cells with X, $, or % (these are goal positions)
   - Boxes: cells with @ or $ (these are box positions)
   - Player: cell with * or %
2. Count goals and boxes - they should be equal.
3. Identify which goals are empty (X or %) - these need boxes pushed to them.
4. Plan moves to push boxes from non-goal positions (@) to empty goals.
5. **IMPORTANT**: When planning a path, verify EACH move step-by-step:
   - For each move, check what is in the destination cell
   - If it's a wall (+), the move is invalid
   - If it's a box (@ or $), you can only move there if you're pushing and the cell beyond the box is empty or a goal
   - If it's empty (-) or goal (X), you can move there freely

**Move Verification Process:**
Before finalizing your answer, trace through EVERY move in your solution:
1. Start with the initial player position
2. For each move (L/R/U/D):
   - Calculate the target cell
   - Check if target is empty/goal → player moves there
   - Check if target has a box → verify cell beyond box is empty/goal, then push occurs
   - Check if target is wall → INVALID, find alternative
3. After all moves, verify all goals have boxes

**Common Mistakes to Avoid:**
- Do NOT assume the puzzle is solved just because you see a `$`. There may be additional goals marked with `X` or `%` that still need boxes. Always count ALL goals and ALL boxes.
- Do NOT miscalculate column positions - be very careful when parsing grids with spaces.
- Do NOT write moves that push boxes into walls or other boxes.
- Do NOT confuse the direction needed to push a box (to push a box UP, the player must be BELOW the box and move UP).
- ALWAYS trace your solution move-by-move to verify it works before submitting.

**Output Format:**
- Provide your reasoning showing the grid analysis, positions identified, and move sequence logic.
- Trace through your proposed solution step-by-step to verify each move is valid.
- End with `<answer>MOVES</answer>` where MOVES is your solution string (e.g., `<answer>DRRRDLDLU</answer>`).
- If the puzzle is already solved with no moves needed, output `<answer></answer>`.
"""
# Set to true to run baseline eval before training (compares student vs teacher)
eval_baseline = true

# Environment (can be any env - this example uses Sokoban)
[[orchestrator.env]]
id = "sokoban-env"
args = { num_train_examples = 5000, num_eval_examples = 250, seed = 42, min_w = 4, max_w = 8, min_h = 4, max_h = 8, min_boxes = 3, max_boxes = 8, max_depth = 80 }

# ============================================================================
# Inference Config
# ============================================================================
[inference]
# gpu_memory_utilization = 0.9

# ============================================================================
# Eval Config
# ============================================================================
[orchestrator.eval]
num_examples = 250
rollouts_per_example = 1
eval_base_model = false
interval = 1000 # we dont need intermediate evals

[orchestrator.eval.sampling]
max_tokens = 4096
temperature = 1.0

[[orchestrator.eval.env]]
id = "sokoban-env"
args = { num_train_examples = 5000, num_eval_examples = 250, seed = 42, min_w = 4, max_w = 8, min_h = 4, max_h = 8, min_boxes = 3, max_boxes = 8, max_depth = 80 }


# [inference.parallel]
# tp = 1                    # Tensor parallelism for large models
