# Baseline RL (no context distillation)
# Same setup as rl.toml but without teacher â€” pure task reward only.
#
# GPU allocation (3 GPUs):
#   GPU 0,1: Student inference (vLLM, DP=2)
#   GPU 2,3: Trainer (FSDP2)
#
# Run with:
#   uv run rl @ experiments/context_distill/baseline.toml \
#     --inference_gpu_ids 0,1 \
#     --trainer_gpu_ids 2,3 \
#     --inference.parallel.dp 2

# ============================================================================
# Training
# ============================================================================
max_steps = 400
seq_len =  16884 #8192              # Max sequence length (also micro batch token capacity)
output_dir = "outputs/baseline"

# ============================================================================
# Model
# ============================================================================
[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

# ============================================================================
# Checkpointing
# ============================================================================
[ckpt]
interval = 50              # Save every N steps

# ============================================================================
# Logging
# ============================================================================
[wandb]
project = "sokoban-rl"
name = "sokoban-baseline"

# ============================================================================
# Trainer Config
# ============================================================================
[trainer.model]
impl = "hf"

[trainer.model.ac]          # Activation checkpointing
freq = 1

[trainer.model.lora]        # LoRA settings
rank = 32
alpha = 64
train_lm_head = true

[trainer.ckpt.weights]
save_adapter_separately = true

[trainer.optim]
lr = 1e-5
betas1 = 0.9
betas2 = 0.9
weight_decay = 0.0
max_norm = 1.0              # Gradient clipping

[trainer.loss]
adv_tau = 1.0               # Weight for task reward
teacher_tau = 0.0           # No distillation
student_tau = 0.0           # No entropy bonus

# ============================================================================
# Orchestrator Config
# ============================================================================
[orchestrator]
batch_size = 256            # Global batch size (total samples per step)
rollouts_per_example = 16 #8    # Rollouts per prompt

[orchestrator.sampling]
max_tokens = 8192           # Max generation tokens
temperature = 1.0           # Sampling temperature

# Environment
[[orchestrator.env]]
id = "sokoban-env"
args = { num_train_examples = 15000, num_eval_examples = 1000, seed = 40, min_w = 4, max_w = 9, min_h = 4, max_h = 9, min_boxes = 1, max_boxes = 7, max_depth = 80 }

# ============================================================================
# Inference Config
# ============================================================================
[inference]
# gpu_memory_utilization = 0.9

[orchestrator.log]
level = "debug"

# ============================================================================
# Eval Config
# ============================================================================
[orchestrator.eval]
num_examples = 1024
rollouts_per_example = 1
eval_base_model = false
interval = 1000 # we dont need intermediate evals

[orchestrator.eval.sampling]
max_tokens = 8192
temperature = 1.0

[[orchestrator.eval.env]]
id = "sokoban-env"
args = { num_train_examples = 5000, num_eval_examples = 1000, seed = 42, min_w = 4, max_w = 9, min_h = 4, max_h = 9, min_boxes = 1, max_boxes = 7, max_depth = 80 }
